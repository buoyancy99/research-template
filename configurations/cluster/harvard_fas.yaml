defaults:
  - base_slurm

# harvard fas cluster, an example for multi node training

params:
  env_name: null # change this to your conda env name on cluster
  partition: null # e.g. kempner_h100
  account: null # e.g. kempner_sham_lab
  num_gpus: 4
  ntasks_per_node: $(cluster.params.num_gpus)
  num_cpus: 48 # we automatically calculate cpus_per_task for you
  memory: 256G

launch_template: |
  #!/bin/bash

  #SBATCH -J {name}
  #SBATCH -o {log_dir}/out_%j.out
  #SBATCH -e {log_dir}/error_%j.err
  #SBATCH --mail-user={email}
  #SBATCH --mail-type=FAIL
  #SBATCH --account={account}
  #SBATCH --partition={partition}
  #SBATCH --nodes=${experiment.num_nodes}
  #SBATCH --ntasks-per-node=${ntasks_per_node}
  #SBATCH --gres=gpu:{num_gpus}
  #SBATCH --cpus-per-task={cpus_per_task}
  #SBATCH --mem={memory}
  #SBATCH --time={time}

  # export NCCL_DEBUG=INFO
  # export PYTHONFAULTHANDLER=1

  cd {project_root}
  module load Mambaforge
  mamba activate {env_name}
  
  srun python -m main {python_args}
