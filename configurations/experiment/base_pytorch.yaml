# inherites from base_experiment.yaml
# most of the options have docs at https://lightning.ai/docs/pytorch/stable/common/trainer.html

defaults:
  - base_experiment

tasks: [train] # tasks to run sequantially, change when your project has multiple stages and you want to run only a subset of them.

training:
  precision: 16-mixed # set float precision, 16-mixed is faster while 32 is more stable
  batch_size: 16 # training batch size, will be splitted into each GPU
  max_epochs: 1000 # set to -1 to train forever
  max_steps: -1 # set to -1 to train forever, will override max_epochs
  max_time: null # set to something like "00:12:00:00" to enable
  data:
    num_workers: 16 # number of CPU threads for data preprocessing.
    shuffle: True # whether training data will be shuffled
  optim:
    accumulate_grad_batches: 1 # accumulate gradients for n batches before backprop
  checkpointing:
    every_n_train_steps: 5000 # save a checkpoint every n train steps

validation:
  batch_size: 16 # validation batch size
  val_check_interval: 2000 # if you want to do validation more than 1 time per training epoch, can be float (fraction of epoches) or int (steps)
  check_val_every_n_epoch: 1 # if you want to do validation every n epoches, requires val_check_interval to be null.
  limit_batch: null # if not null, limit the number of batches to use for validation.
  data:
    num_workers: 16 # number of CPU threads for data preprocessing, for validation.
    shuffle: False # whether validation data will be shuffled
